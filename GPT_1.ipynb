{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5cf61c4b",
      "metadata": {
        "id": "5cf61c4b"
      },
      "source": [
        "# Programming Task Description\n",
        "\n",
        "## Programming Task: Implementing a Character-Level GPT Model\n",
        "\n",
        "### Introduction\n",
        "In this task, you will create a Python script using PyTorch to implement a simplified GPT (Generative Pre-trained Transformer) model for character-level language modeling. The model will be trained on the text in input.txt to predict the next character in a sequence and generate new text based on a given context. The architecture follows the decoder part of the transformer model from the \"Attention is All You Need\" paper by Vaswani et al., focusing on masked multi-head self-attention to ensure predictions depend only on previous positions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8537c2f6",
      "metadata": {
        "id": "8537c2f6"
      },
      "source": [
        "## Task Description\n",
        "### Your goal is to write a Python jupyter notebook that:\n",
        "\n",
        "1. Reads and processes the text from input.txt.\n",
        "2. Implements a decoder-only transformer model with manual attention mechanisms.\n",
        "3. Trains the model on the processed data.\n",
        "4. Generates new text using the trained model.\n",
        "\n",
        "You will use PyTorch and implement the attention mechanism from scratch, following the decoder structure outlined in the \"Attention is All You Need\" paper."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df79368",
      "metadata": {
        "id": "2df79368"
      },
      "source": [
        "### Step-by-step Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5d36b1",
      "metadata": {
        "id": "1d5d36b1"
      },
      "source": [
        "1. Data Preparation\n",
        "* Read all text from input.txt using UTF-8 encoding.\n",
        "* Create a sorted list of unique characters (vocabulary) from the text.\n",
        "* Build two dictionaries:\n",
        "    * stoi: Maps characters to integers (e.g., 'a' -> 0).\n",
        "    * itos: Maps integers to characters (e.g., 0 -> 'a').\n",
        "* Define functions:\n",
        "    * encode(s): Converts a string to a list of integers using stoi.\n",
        "    * decode(l): Converts a list of integers to a string using itos.\n",
        "* Encode the entire text into a tensor of integers using torch.tensor.\n",
        "* Split the data: 90% for training, 10% for validation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d37a3e46",
      "metadata": {
        "id": "d37a3e46"
      },
      "source": [
        "2. Data Loading\n",
        "* Implement a function get_batch(split):\n",
        "    * Input: split is either 'train' or 'val'.\n",
        "    * Select the appropriate dataset (training or validation).\n",
        "    * Randomly sample batch_size starting indices, ensuring each sequence fits within block_size.\n",
        "* Return:\n",
        "    * x: A tensor of shape (batch_size, block_size) with input sequences.\n",
        "    * y: A tensor of shape (batch_size, block_size) with target sequences (shifted by one position).\n",
        "* Move tensors to the device (CPU or GPU)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f100337",
      "metadata": {
        "id": "6f100337"
      },
      "source": [
        "3. Model Architecture\n",
        "* Implement the following components in a decoder-only transformer:\n",
        "    * Embedding Layers:\n",
        "        * Token embedding: nn.Embedding(vocab_size, n_embd) for character indices.\n",
        "        * Position embedding: nn.Embedding(block_size, n_embd) for positions 0 to block_size-1.\n",
        "    * Transformer Blocks:\n",
        "        * Each block includes:\n",
        "            * Masked Multi-Head Self-Attention:\n",
        "                * Implement manually (do not use nn.MultiheadAttention).\n",
        "                * For each head:\n",
        "                    * Linear layers for queries (Q), keys (K), and values (V).\n",
        "                    * Scaled dot-product attention: attention = softmax((Q @ K.T) / sqrt(d_k)) @ V.\n",
        "                    * Mask future positions with a lower triangular matrix (e.g., tril) by setting future weights to -inf before softmax.\n",
        "                * Concatenate heads and apply a projection layer.\n",
        "            * Feed-Forward Network: nn.Linear(n_embd, 4 * n_embd) → ReLU → nn.Linear(4 * n_embd, n_embd).\n",
        "            * Layer Normalization: Apply nn.LayerNorm(n_embd) before each sub-layer (pre-norm).\n",
        "            * Residual Connections: Add input to output of each sub-layer.\n",
        "        * Use n_layer blocks in sequence.\n",
        "    * Final Layers:\n",
        "        * nn.LayerNorm(n_embd) for final normalization.\n",
        "        * nn.Linear(n_embd, vocab_size) to produce logits.\n",
        "* Define a GPTLanguageModel class with:\n",
        "    * forward(idx, targets=None): Computes logits and loss (if targets provided).\n",
        "    * generate(idx, max_new_tokens): Autoregressively generates new tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2098832e",
      "metadata": {
        "id": "2098832e"
      },
      "source": [
        "4. Training\n",
        "* Use the AdamW optimizer with learning_rate = 3e-4.\n",
        "* Train for max_iters = 5000 iterations.\n",
        "* Estimate and print training and validation losses:\n",
        "* Compute loss using F.cross_entropy on flattened logits and targets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1838a7a8",
      "metadata": {
        "id": "1838a7a8"
      },
      "source": [
        "5. Text Generation\n",
        "* Implement generate(idx, max_new_tokens):\n",
        "    * Start with an initial context idx (shape (B, T)).\n",
        "    * For max_new_tokens steps:\n",
        "        * Crop idx to the last block_size tokens.\n",
        "        * Get logits from forward.\n",
        "        * Apply softmax to the last time step’s logits to get probabilities.\n",
        "        * Sample the next token using torch.multinomial.\n",
        "        * Append the sampled token to idx.\n",
        "    * Return the extended sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d2b8dab",
      "metadata": {
        "id": "0d2b8dab"
      },
      "source": [
        "### Hyperparameters\n",
        "Use these values:\n",
        "\n",
        "* batch_size = 64\n",
        "* block_size = 256\n",
        "* n_embd = 384\n",
        "* n_head = 6\n",
        "* n_layer = 6\n",
        "* dropout = 0.2\n",
        "* learning_rate = 3e-4\n",
        "* max_iters = 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89d1a954",
      "metadata": {
        "id": "89d1a954"
      },
      "source": [
        "### Understanding the Decoder\n",
        "The \"Attention is All You Need\" paper describes a transformer with an encoder and decoder. For this task, you focus on the decoder-only architecture used in GPT:\n",
        "\n",
        "* Masked Self-Attention: Ensures the model only attends to previous positions in the sequence, making it autoregressive. This is achieved by masking future tokens in the attention computation, as shown below:\n",
        "\n",
        "$Attention (Q, K, V) = softmax((Q@K.T)/sqrt(d_{k}) + mask) @V$\n",
        "\n",
        "where $mask$ sets future positions to $-inf$\n",
        "\n",
        "* Decoder Role: In the original paper, the decoder generates output sequences while attending to the encoder’s output. Here, without an encoder, it uses self-attention on the input sequence alone, predicting the next token step-by-step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b33caf5",
      "metadata": {
        "id": "5b33caf5"
      },
      "source": [
        "### Additional Notes\n",
        "* Manual Attention: Implement attention from scratch to understand its mechanics (no pre-built PyTorch modules).\n",
        "* Masking: Use a lower triangular matrix (e.g., torch.tril) to mask future positions.\n",
        "* Device Handling: Set device = 'cuda' if torch.cuda.is_available() else 'cpu' and move tensors/models accordingly.\n",
        "* Dropout: Apply nn.Dropout(dropout) in attention and feed-forward layers for regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0da406",
      "metadata": {
        "id": "9c0da406"
      },
      "source": [
        "### Deliverables\n",
        "A Python script that:\n",
        "* Implements all steps above.\n",
        "* Prints training and validation losses every 500/100? iterations (up to you).\n",
        "* Generates and prints a 500-character sample after training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b7b3ac",
      "metadata": {
        "id": "36b7b3ac"
      },
      "source": [
        "### Evaluation Criteria\n",
        "* Correct data preparation and batch loading.\n",
        "* Accurate implementation of the transformer model, especially masked self-attention.\n",
        "* Successful training with decreasing loss.\n",
        "* Generation of coherent (for character-level) text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7296007",
      "metadata": {
        "id": "f7296007"
      },
      "source": [
        "# Installing Packages\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy pandas"
      ],
      "metadata": {
        "id": "Q1b7yQsL8Efx"
      },
      "id": "Q1b7yQsL8Efx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "---"
      ],
      "metadata": {
        "id": "E9FuuG8Q7T0G"
      },
      "id": "E9FuuG8Q7T0G"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def read_file_utf8(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "# reading text file with utf8 encoding\n",
        "file_path = 'input.txt'\n",
        "text = read_file_utf8(file_path)\n",
        "\n",
        "# sorted list of unique characters from input\n",
        "vocabulary = sorted(list(set(text)))\n",
        "\n",
        "# string to integer mapping\n",
        "stoi = {ch: i for i, ch in enumerate(vocabulary)}\n",
        "\n",
        "# integer to string mapping\n",
        "itos = {i: ch for i, ch in enumerate(vocabulary)}"
      ],
      "metadata": {
        "id": "DPkqUuZ679FV"
      },
      "id": "DPkqUuZ679FV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encode Decode Functions**"
      ],
      "metadata": {
        "id": "Iiy02RJAAh25"
      },
      "id": "Iiy02RJAAh25"
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(s):\n",
        "  output = []\n",
        "  for char in s:\n",
        "    output.append(stoi[char])\n",
        "  return output\n",
        "\n",
        "def decode(i):\n",
        "  output = \"\"\n",
        "  for j in i:\n",
        "    output += itos[j]\n",
        "  return output"
      ],
      "metadata": {
        "id": "OKJNIpuQ-_mN"
      },
      "id": "OKJNIpuQ-_mN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization and Training-Val Split (90/10)**"
      ],
      "metadata": {
        "id": "hd2eEt1rKrKM"
      },
      "id": "hd2eEt1rKrKM"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# character-level tokenization\n",
        "tokens = encode(text)\n",
        "tensor = torch.tensor(tokens) # 1D tensor of size [1115394] elements\n",
        "print(f\"Tensor size: {tensor.shape}\")\n",
        "\n",
        "split_idx = int(0.9 * len(tensor))\n",
        "\n",
        "# split tensor into 90/10 training and validation sets\n",
        "train_set = tensor[:split_idx]\n",
        "print(f\"Training set size: {train_set.shape}\")\n",
        "val_test = tensor[split_idx:]\n",
        "print(f\"Validation set size: {val_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0d5XqT4CFAW",
        "outputId": "c02230d3-e8de-4e73-e734-2ee3307f9b56"
      },
      "id": "p0d5XqT4CFAW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor size: torch.Size([1115394])\n",
            "Training set size: torch.Size([1003854])\n",
            "Validation set size: torch.Size([111540])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading\n",
        "---"
      ],
      "metadata": {
        "id": "P7EamqggKzQ0"
      },
      "id": "P7EamqggKzQ0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Batches**"
      ],
      "metadata": {
        "id": "4fa1Mt1_Ph1A"
      },
      "id": "4fa1Mt1_Ph1A"
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 # num of blocks\n",
        "block_size = 256 # num of chars in a sequence\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "  # returns random int tensor of size batch_size\n",
        "  rand_indices = torch.randint(len(split) - block_size, (batch_size,))\n",
        "\n",
        "  # returns tensors of size (batch_size, block_size)\n",
        "  x = torch.stack([split[i:i+block_size] for i in rand_indices])\n",
        "\n",
        "  # target sequence shifted by 1\n",
        "  y = torch.stack([split[i+1:i+block_size+1] for i in rand_indices])\n",
        "  return x, y\n",
        "\n",
        "x, y = get_batch(train_set)\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rnts05vKwZL",
        "outputId": "6109e855-c519-43de-b497-bf7f9e73fe57"
      },
      "id": "-Rnts05vKwZL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 256]) torch.Size([64, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Tensor to Device (GPU or CPU)**\n"
      ],
      "metadata": {
        "id": "2xG6W6MXQALZ"
      },
      "id": "2xG6W6MXQALZ"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tensor = tensor.to(device)"
      ],
      "metadata": {
        "id": "NuCrJSpePT7f"
      },
      "id": "NuCrJSpePT7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture\n",
        "---"
      ],
      "metadata": {
        "id": "YVsZ0lBETZi5"
      },
      "id": "YVsZ0lBETZi5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Layer**"
      ],
      "metadata": {
        "id": "QRXFBRo8JXqi"
      },
      "id": "QRXFBRo8JXqi"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, block_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape  # x is (batch, seq_len) of token indices\n",
        "        tok_emb = self.token_embedding(x)         # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))  # (T, n_embd)\n",
        "        return tok_emb + pos_emb  # (B, T, n_embd)"
      ],
      "metadata": {
        "id": "faN1i2lSJVAo"
      },
      "id": "faN1i2lSJVAo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer Block**"
      ],
      "metadata": {
        "id": "4S0dD9Jp_Yuh"
      },
      "id": "4S0dD9Jp_Yuh"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from math import sqrt\n",
        "\n",
        "class Transformer_Block(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # n_heads hyperparameter\n",
        "        self.n_head = n_head\n",
        "\n",
        "        # embedding layer\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # (num of embeddings, dimensions)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # linear layers for multi_head attention\n",
        "        self.q_proj = nn.Linear(n_embd, n_embd)\n",
        "        self.k_proj = nn.Linear(n_embd, n_embd)\n",
        "        self.v_proj = nn.Linear(n_embd, n_embd)\n",
        "        self.output_proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        # feed forward layer\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd)\n",
        "        )\n",
        "\n",
        "        # norm layers\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "    def masked_attention(self, Q, K, V):\n",
        "        \"\"\"\n",
        "        Q, K, V: (batch, n_head, seq_len, head_dim)\n",
        "        Returns: (batch, n_head, seq_len, head_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        # compute scores\n",
        "        d_k = Q.shape[-1]\n",
        "        scores = (Q @ K.transpose(-2, -1)) / sqrt(d_k)\n",
        "\n",
        "        # creates a diagonal of -inf values to mask future comparisions\n",
        "        T = scores.size(-1)\n",
        "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(Q.device).bool()\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        # apply softmax\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        return weights @ V # returns tensor\n",
        "\n",
        "    def multihead_attention(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, tokens, embeddings)\n",
        "        Returns: (batch, tokens, embeddings)\n",
        "        \"\"\"\n",
        "        # batch(num of sequences), tokens(num of tokens), embeddings(size of vector for each token)\n",
        "        B, T, E = x.shape\n",
        "\n",
        "        # divide embeddings for parallel attention\n",
        "        head_dim = E // self.n_head\n",
        "\n",
        "        # passing Q, K, V through linear layers to transform their shapes\n",
        "        Q = self.q_proj(x) # (B, T, E)\n",
        "        K = self.k_proj(x)\n",
        "        V = self.v_proj(x)\n",
        "\n",
        "        # using view to add n_head as a dimension\n",
        "        Q = Q.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "        K = K.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "        V = V.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "\n",
        "        output = self.masked_attention(Q, K, V)                     # (B, n_head, T, head_dim)\n",
        "        output = output.transpose(1, 2).contiguous().view(B, T, E)  # (B, T, E)\n",
        "\n",
        "        # go through final linear layer\n",
        "        output = self.output_proj(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # masked multihead attention -> add & norm\n",
        "        attn_out = self.multihead_attention(x)\n",
        "        x = self.ln1(x + attn_out)\n",
        "\n",
        "        # feedforward -> add & norm\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = self.ln2(x + ff_out)\n",
        "        return x"
      ],
      "metadata": {
        "id": "INxNoVCST_t2"
      },
      "id": "INxNoVCST_t2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPTLanguageModel:**"
      ],
      "metadata": {
        "id": "PU1YmJNy_tPc"
      },
      "id": "PU1YmJNy_tPc"
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = TransformerEmbedding(vocab_size, n_embd, block_size)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Transformer_Block(\n",
        "                vocab_size=vocab_size,         # or omit if not used in the block\n",
        "                n_embd=n_embd,\n",
        "                n_head=n_head,\n",
        "                n_layer=n_layer,               # can ignore if not used in block\n",
        "                block_size=block_size,\n",
        "                dropout=dropout\n",
        "            )\n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)    # (B, T, n_embd)\n",
        "        x = self.blocks(x)       # (B, T, n_embd)\n",
        "        x = self.ln_f(x)         # (B, T, n_embd)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "x9cpaNdp_u41"
      },
      "id": "x9cpaNdp_u41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel(\n",
        "    vocab_size=len(vocabulary),\n",
        "    n_embd=384,\n",
        "    n_head=6,\n",
        "    n_layer=6,\n",
        "    block_size=256,\n",
        "    dropout=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "-Ydtw8JsJ9ZZ"
      },
      "id": "-Ydtw8JsJ9ZZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}